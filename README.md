# egap-qnn-mlp
This repository contains code for training and evaluating two machine learning models to predict the electronic bandgap (`egap`) of materials:

- **MLP**: A classical multilayer perceptron implemented in PyTorch.
- **Hybrid QNN**: A hybrid quantum‑classical neural network using PennyLane and PyTorch.

All models use the same set of features derived from crystal structures:

- **Lattice parameters**: `a`, `b`, `c`
- **Lattice angles**: `alpha`, `beta`, `gamma`
- **Density**
- **Enthalpy of formation per atom**: `enthalpy_formation_atom`
- **Spacegroup (categorical)**: `spacegroup_relax` (1–230)
- **Volume of unit cell**: `volume_cell`
- **Derived features**:
  - `avg_en_dev` — average electronegativity deviation
  - `avg_mendeleev` — average Mendeleev number
  - `p_fraction` — fraction of p‑electrons over total valence electrons
  - `group_std` — standard deviation of periodic table groups
  - `num_unique_elements`
  - `num_constituent_atoms`

## Repository Structure

```
├── fetching_data.py           # Async script to fetch and preprocess raw data from AFLOW
├── element_properties_cache/  # Joblib cache for element property lookups
├── data/
│   └── raw_data.csv           # Raw CSV generated by fetching_data.py
├── split_data.py              # Hybrid splitter script (Kennard–Stone + clustering + stratification)
├── train.csv                  # Training split
├── val.csv                    # Validation split
├── test.csv                   # Test split
├── mlp.py                     # Multilayer Perceptron implementation
├── hybrid_qnn.py              # Hybrid quantum‑classical QNN implementation
└── README.md                  # This file
```

## Prerequisites

- Python **3.8** – **3.10** (due to PennyLane and TensorFlow numpy compatibility)
- **pip**

## Installation

1. Clone this repository:

   ```bash
   git clone https://github.com/yourusername/materials-bandgap-prediction.git
   cd materials-bandgap-prediction
   ```

2. Create a virtual environment (recommended) and activate it:

   ```bash
   python -m venv .venv
   source .venv/bin/activate      # macOS/Linux
   .venv\\Scripts\\activate     # Windows
   ```

3. Install dependencies:

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

   If you encounter numpy version conflicts, install:

   ```bash
   pip install numpy==1.26.4
   ```

## Data Preparation

Raw data is fetched via the `fetching_data.py` script, which:  
- Queries the AFLOW REST API asynchronously in optimized batches using asyncio and aiohttp.  
- Precomputes and caches elemental properties in the `element_properties_cache/` folder via joblib for fast lookups (e.g., electronegativity, valence, Mendeleev number).  
- Processes each entry with multiprocessing, parses composition, computes derived features (e.g., `avg_en_dev`, `avg_mendeleev`, `p_fraction`, `group_std`, `num_unique_elements`, `num_constituent_atoms`), and writes to a raw CSV (`raw_data.csv`).  

To generate the master CSV (`raw_set.csv` or `raw_data.csv`), run:
```bash
python fetching_data.py --n_entries 15000 --csv_file raw_data.csv --batch_size 150 \
    --max_concurrent 10 --max_processes 8 --buffer_size 1000
```

Before splitting, ensure the `element_properties_cache/` directory (created by joblib) is present in the repository root.  

Once you have `raw_data.csv`, split it into train/validation/test using:
```bash
python split_data.py raw_data.csv train.csv val.csv test.csv \
    --test_frac 0.2 --val_frac 0.2 --n_clusters 5
```

## Usage

### 1. Train and evaluate the MLP model

```bash
python mlp.py
```

This will:

- Load `train.csv`, `val.csv`, `test.csv`
- Preprocess numeric and categorical features (standard scaling + one‑hot encoding)
- Train a 3‑layer MLP (256 → 128 → 64 → 1)
- Print test metrics: MSE, RMSE, MAE, R²
- Save a scatter plot `egap_predictions.png` in the project root

### 2. Train and evaluate the Hybrid QNN model

```bash
python hybrid_qnn.py
```

This will:

- Load and preprocess data (same as MLP)
- Build a hybrid network with a 2‑layer parameterized quantum circuit (4 qubits)
- Train with early stopping
- Print test metrics
- Save `egap_predictions.png` (overwrites MLP plot)

## Metrics

Both scripts report:

- **MSE**: Mean Squared Error
- **RMSE**: Root Mean Squared Error
- **MAE**: Mean Absolute Error
- **R²**: Coefficient of determination

### Runtime Comparison

You can compare total training time by wrapping each script call with `/usr/bin/time -v` or Python’s `time` module.

## Visualization

- `egap_predictions.png`: Predicted vs. Actual scatter plot with R² annotation.

## Contributing

1. Fork the repository
2. Create a new branch (`git checkout -b feature/your-feature`)
3. Commit your changes (`git commit -m 'Add your feature'`)
4. Push to your branch (`git push origin feature/your-feature`)
5. Open a Pull Request

## Dependencies

Create a file `requirements.txt` in the repository root with the following contents:
```text
# Numeric & data handling
numpy>=1.26.0,<2.2.0
pandas>=1.5.0

# Machine learning & preprocessing
scikit-learn>=1.2.2
modAL>=0.7.0

# Deep learning
torch>=1.12.0

# Quantum computing
pennylane>=0.40.0

# Materials data sources & processing
aflow>=1.0.0
pymatgen>=2023.0.0
mendeleev>=0.12.0

# Async & caching
aiohttp>=3.8.0
joblib>=1.2.0

# Visualization
matplotlib>=3.5.0
```

Install all dependencies with:
```bash
pip install -r requirements.txt
```



